{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ei1g6j1nigB6"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkxlDcE7YBl7"
      },
      "source": [
        "### MelanomaDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox4PCjZCg6Cq"
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "    '''Melanoma Dataset'''\n",
        "\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.image_paths = self.df.filepath.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            res = self.transform(image=image)\n",
        "            image = res['image'].astype(np.float32)\n",
        "        else:\n",
        "            image = image.astype(np.float32)\n",
        "\n",
        "        image = image.transpose(2, 0, 1)\n",
        "\n",
        "        label = torch.tensor(self.df.target.values[index])\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lQcsdA-42wu"
      },
      "source": [
        "### AttConv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdjfAxzj40oO"
      },
      "source": [
        "class AttConv(nn.Module):\n",
        "    def __init__(self, num_in_chan=1, num_out_chan=6, kernel_size=5, stride=1):\n",
        "        super(AttConv, self).__init__()\n",
        "        self.kernel_size=kernel_size\n",
        "        self.num_in_chan=num_in_chan\n",
        "        self.num_out_chan=num_out_chan\n",
        "        self.stride=stride\n",
        "        self.BU_weights = nn.Parameter(torch.HalfTensor(1,num_in_chan*kernel_size**2, 1, num_out_chan))\n",
        "        init.kaiming_uniform_(self.BU_weights, a=np.sqrt(5))\n",
        "        self.TD_weights = nn.Parameter(self.BU_weights.data.detach().clone())\n",
        "        \n",
        "        self.BU_bias = nn.Parameter(torch.randn(1,1,1,num_out_chan)*0.1)\n",
        "        self.TD_bias = nn.Parameter(torch.randn(1,num_in_chan,1,1)*0.1)\n",
        "        \n",
        "    def normalize_att_weights(self, in_spat_dim):\n",
        "        batch_size = self.att_weights.shape[0]        \n",
        "        num_wins = self.att_weights.shape[-1]\n",
        "        aw_sum = F.unfold(F.fold(self.att_weights, in_spat_dim, self.kernel_size,stride=self.stride), self.kernel_size, stride=self.stride) #Fold into image domain (which automatically computes the sum per pixel), and then unfold again into conv windows    \n",
        "        self.att_weights = self.att_weights/aw_sum #Normalize weights by their sum over possible parents\n",
        "        self.att_weights = self.att_weights.view(batch_size, 1, self.kernel_size**2, num_wins).expand(batch_size, self.num_in_chan, self.kernel_size**2, num_wins).reshape(batch_size, self.num_in_chan*self.kernel_size**2, num_wins)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, num_iter=4):\n",
        "        batch_size = x.shape[0]\n",
        "        device = x.device\n",
        "        in_spat_dim = list(x.shape[-2:])                \n",
        "        assert in_spat_dim[0]==in_spat_dim[1], 'Only square images are supported'\n",
        "        x_wins = F.unfold(x.view(batch_size,self.num_in_chan,*in_spat_dim), self.kernel_size, stride=self.stride)\n",
        "        x_wins = x_wins.type(torch.half)\n",
        "        out_spat_dim = np.int(np.sqrt(x_wins.shape[-1]))\n",
        "        self.att_weights = torch.ones([batch_size, self.kernel_size**2, x_wins.shape[-1]], device=device, dtype=torch.half)        \n",
        "        self.normalize_att_weights(in_spat_dim)\n",
        "        \n",
        "        \n",
        "        for i in range(num_iter):\n",
        "            y = F.relu((x_wins.unsqueeze(-1)*self.att_weights.unsqueeze(-1)*self.BU_weights).sum(1,True) + self.BU_bias)\n",
        "            pred = (y*self.TD_weights).sum(-1).view(batch_size,self.num_in_chan,self.kernel_size**2, -1) + self.TD_bias            \n",
        "            self.att_weights = ((pred*x_wins.view(batch_size,self.num_in_chan,self.kernel_size**2, -1)).sum(1) / np.sqrt(self.num_in_chan)).exp()\n",
        "            self.normalize_att_weights(in_spat_dim)   \n",
        "\n",
        "        y = y.view(batch_size, out_spat_dim, out_spat_dim, self.num_out_chan).permute(0,3,1,2)\n",
        "        \n",
        "        return y\n",
        "\n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "811H3yA7z0US",
        "outputId": "a0e9217b-b92a-43fd-9e42-ed9ffd3c4516"
      },
      "source": [
        "plt.style.use(['classic'])\n",
        "np.random.seed(13)\n",
        "torch.manual_seed(13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9ef69d3e70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f26pCtPOjLd_"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ugjcnRVxFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b29e2ae-bcb7-449c-ffa9-0281e6bd1d90"
      },
      "source": [
        "image_size = 128\n",
        "\n",
        "train_transforms = albumentations.Compose([\n",
        "        albumentations.Transpose(p=0.5),\n",
        "        albumentations.VerticalFlip(p=0.5),\n",
        "        albumentations.HorizontalFlip(p=0.5),\n",
        "        albumentations.RandomBrightness(limit=0.2, p=0.75),\n",
        "        albumentations.RandomContrast(limit=0.2, p=0.75),\n",
        "        albumentations.OneOf([\n",
        "            albumentations.MotionBlur(blur_limit=5),\n",
        "            albumentations.MedianBlur(blur_limit=5),\n",
        "            albumentations.GaussianBlur(blur_limit=5),\n",
        "            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.OneOf([\n",
        "            albumentations.OpticalDistortion(distort_limit=1.0),\n",
        "            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
        "            albumentations.ElasticTransform(alpha=3),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n",
        "        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
        "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
        "        albumentations.Resize(image_size, image_size),\n",
        "        albumentations.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n",
        "        albumentations.Normalize()\n",
        "    ])\n",
        "\n",
        "test_transforms = albumentations.Compose([\n",
        "        albumentations.Resize(image_size, image_size),\n",
        "        albumentations.Normalize()\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1701: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1727: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1852: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
            "  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnH-5GV9V4u_"
      },
      "source": [
        "train_set = MelanomaDataset(train_data, transform=train_transforms)\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=2)\n",
        "\n",
        "valid_set = MelanomaDataset(valid_data, transform=test_transforms)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGaS5HOiWBr2"
      },
      "source": [
        "epochs = 5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "# loss = torch.nn.CrossEntropyLoss(weight=class_weight_ts)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "loss = loss.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzu44M6MWCmS"
      },
      "source": [
        "running_loss = np.zeros((epochs, len(train_loader)))\n",
        "running_train_acc = np.zeros(epochs)\n",
        "running_valid_acc = np.zeros(epochs)\n",
        "# auc = np.zeros(epochs)\n",
        "\n",
        "best_accuracy = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nrV77KyS6i"
      },
      "source": [
        "#### Attention with upsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oomvA3Bf9lyF",
        "outputId": "cf8d9523-e6bc-4697-cb5e-876f082659a4"
      },
      "source": [
        "image_size = 128\n",
        "\n",
        "train_transforms = albumentations.Compose([\n",
        "        albumentations.Transpose(p=0.5),\n",
        "        albumentations.VerticalFlip(p=0.5),\n",
        "        albumentations.HorizontalFlip(p=0.5),\n",
        "        albumentations.RandomBrightness(limit=0.2, p=0.75),\n",
        "        albumentations.RandomContrast(limit=0.2, p=0.75),\n",
        "        albumentations.OneOf([\n",
        "            albumentations.MotionBlur(blur_limit=5),\n",
        "            albumentations.MedianBlur(blur_limit=5),\n",
        "            albumentations.GaussianBlur(blur_limit=5),\n",
        "            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.OneOf([\n",
        "            albumentations.OpticalDistortion(distort_limit=1.0),\n",
        "            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
        "            albumentations.ElasticTransform(alpha=3),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n",
        "        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
        "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
        "        albumentations.Resize(image_size, image_size),\n",
        "        albumentations.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n",
        "        albumentations.Normalize()\n",
        "    ])\n",
        "\n",
        "test_transforms = albumentations.Compose([\n",
        "        albumentations.Resize(image_size, image_size),\n",
        "        albumentations.Normalize()\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1701: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1727: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1852: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
            "  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:645: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqGneRtW9ljF"
      },
      "source": [
        "# train_set = MelanomaDataset(train_data, transform=train_transforms)\n",
        "# train_loader = DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=2)\n",
        "\n",
        "# valid_set = MelanomaDataset(valid_data, transform=test_transforms)\n",
        "# valid_loader = DataLoader(valid_set, batch_size=16, shuffle=False, num_workers=1)\n",
        "# --\n",
        "\n",
        "train_set = MelanomaDataset(train_data, transform=train_transforms)\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=2)\n",
        "\n",
        "valid_set = MelanomaDataset(valid_data, transform=test_transforms)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P85AkJ0SFuVB",
        "outputId": "46b1a69a-6037-499b-ea89-86a80e3be643"
      },
      "source": [
        "class AttAllConvNet(nn.Module):\n",
        "    def __init__(self, input_size, n_classes=10, **kwargs):\n",
        "        super(AttAllConvNet, self).__init__()\n",
        "        # self.conv1 = nn.Conv2d(input_size, 96, 3, padding=1)\n",
        "        self.conv1 = AttConv(num_in_chan=3, num_out_chan=96, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(96, 96, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(96, 96, 3, padding=1, stride=2)\n",
        "        self.conv4 = nn.Conv2d(96, 192, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(192, 192, 3, padding=1, stride=2)\n",
        "        self.conv7 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(192, 192, 1)\n",
        "\n",
        "        self.class_conv = nn.Conv2d(192, n_classes, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_drop = F.dropout(x, .2)\n",
        "        conv1_out = self.conv1(x_drop)\n",
        "        conv2_out = F.relu(self.conv2(conv1_out))\n",
        "        conv3_out = F.relu(self.conv3(conv2_out))\n",
        "        conv3_out_drop = F.dropout(conv3_out, .5)\n",
        "        conv4_out = F.relu(self.conv4(conv3_out_drop))\n",
        "        conv5_out = F.relu(self.conv5(conv4_out))\n",
        "        conv6_out = F.relu(self.conv6(conv5_out))\n",
        "        conv6_out_drop = F.dropout(conv6_out, .5)\n",
        "        conv7_out = F.relu(self.conv7(conv6_out_drop))\n",
        "        conv8_out = F.relu(self.conv8(conv7_out))\n",
        "\n",
        "        class_out = F.relu(self.class_conv(conv8_out))\n",
        "        pool_out = F.adaptive_avg_pool2d(class_out, 1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        return pool_out\n",
        "\n",
        "\n",
        "att_allconv = AttAllConvNet(input_size=3, n_classes=2)\n",
        "att_allconv.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttAllConvNet(\n",
              "  (conv1): AttConv()\n",
              "  (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "  (conv4): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv6): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "  (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv8): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (class_conv): Conv2d(192, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEy3lV3sElh5"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9Zs6hEH-FFL",
        "outputId": "2f8365f0-5c00-4282-9c42-d2a3f2a67c45"
      },
      "source": [
        "from torchsummary import summary\n",
        "att_allconv.to(device)\n",
        "summary(att_allconv, (3,128,128))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           AttConv-1         [-1, 96, 126, 126]               0\n",
            "            Conv2d-2         [-1, 96, 126, 126]          83,040\n",
            "            Conv2d-3           [-1, 96, 63, 63]          83,040\n",
            "            Conv2d-4          [-1, 192, 63, 63]         166,080\n",
            "            Conv2d-5          [-1, 192, 63, 63]         331,968\n",
            "            Conv2d-6          [-1, 192, 32, 32]         331,968\n",
            "            Conv2d-7          [-1, 192, 32, 32]         331,968\n",
            "            Conv2d-8          [-1, 192, 32, 32]          37,056\n",
            "            Conv2d-9            [-1, 2, 32, 32]             386\n",
            "================================================================\n",
            "Total params: 1,365,506\n",
            "Trainable params: 1,365,506\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 42.31\n",
            "Params size (MB): 5.21\n",
            "Estimated Total Size (MB): 47.70\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g441gAmylcVB"
      },
      "source": [
        "pd.DataFrame(list(running_loss.transpose()), columns=['Epoch 1']).to_csv(\n",
        "    os.path.join(model_dir, 'running_loss_attallconv.csv'), index=False)\n",
        "pd.DataFrame(running_valid_acc).to_csv(\n",
        "    os.path.join(model_dir, 'val_acc_resnet_attallconv.csv'), index=False)\n",
        "pd.DataFrame(running_train_acc).to_csv(\n",
        "    os.path.join(model_dir, 'train_acc_resnet_attallconv.csv'), index=False)\n",
        "\n",
        "result = pd.read_csv('/content/drive/MyDrive/thesis/models/test_attallconv_upsampled.csv')\n",
        "y_true = result['True Label'].values\n",
        "y_pred = result['Predicted Label'].values\n",
        "\n",
        "cfm = confusion_matrix(y_true, y_pred)\n",
        "print(cfm)\n",
        "plt.title('Melanoma detection with 128 image data')\n",
        "sns.heatmap(cfm, annot=True, cmap='Blues', xticklabels=['pred_Benign', 'pred_Malignant'], yticklabels=['Benign','Malignant'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKuqzcmiSRtU",
        "outputId": "3a5cff5c-6c07-4b44-de79-6d1fef435c15"
      },
      "source": [
        "att_model = models.alexnet(pretrained=False)\n",
        "att_model.features = nn.Sequential(\n",
        "    # nn.BatchNorm2d(3),\n",
        "    AttConv(3, 32, 5),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    nn.Conv2d(32, 96, kernel_size=5, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    nn.Conv2d(96, 192, kernel_size=3, padding=1), nn.ReLU(inplace=True), \n",
        "    nn.Conv2d(192, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    )\n",
        "\n",
        "att_model.classifier = nn.Sequential(\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(in_features=128*6*6, out_features=128*4*4, bias=True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(in_features=128*4*4, out_features=128*4*4, bias=True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(in_features=128*4*4, out_features=2, bias=True),\n",
        "  )\n",
        "\n",
        "att_model.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): AttConv()\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (2): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=2048, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrR3vH3We-mR",
        "outputId": "107f7851-68ae-42de-e5e6-0f83598b7ac1"
      },
      "source": [
        "summary(att_model, (3,128,128))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           AttConv-1         [-1, 32, 124, 124]               0\n",
            "         MaxPool2d-2           [-1, 32, 61, 61]               0\n",
            "            Conv2d-3           [-1, 96, 59, 59]          76,896\n",
            "              ReLU-4           [-1, 96, 59, 59]               0\n",
            "         MaxPool2d-5           [-1, 96, 29, 29]               0\n",
            "            Conv2d-6          [-1, 192, 29, 29]         166,080\n",
            "              ReLU-7          [-1, 192, 29, 29]               0\n",
            "            Conv2d-8          [-1, 128, 29, 29]         221,312\n",
            "              ReLU-9          [-1, 128, 29, 29]               0\n",
            "           Conv2d-10          [-1, 128, 29, 29]         147,584\n",
            "             ReLU-11          [-1, 128, 29, 29]               0\n",
            "        MaxPool2d-12          [-1, 128, 14, 14]               0\n",
            "AdaptiveAvgPool2d-13            [-1, 128, 6, 6]               0\n",
            "          Dropout-14                 [-1, 4608]               0\n",
            "           Linear-15                 [-1, 2048]       9,439,232\n",
            "             ReLU-16                 [-1, 2048]               0\n",
            "          Dropout-17                 [-1, 2048]               0\n",
            "           Linear-18                 [-1, 2048]       4,196,352\n",
            "             ReLU-19                 [-1, 2048]               0\n",
            "           Linear-20                    [-1, 2]           4,098\n",
            "================================================================\n",
            "Total params: 14,251,554\n",
            "Trainable params: 14,251,554\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 16.47\n",
            "Params size (MB): 54.37\n",
            "Estimated Total Size (MB): 71.02\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXn5Od66t_9C"
      },
      "source": [
        "# sys.stdout = open(os.path.join(model_dir, 'AttConv_output(1).txt'), 'w')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_correct = 0\n",
        "    valid_correct = 0\n",
        "    scheduler.step()\n",
        "\n",
        "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        output = att_model(images).to(device)\n",
        "        # Calculate loss\n",
        "        err = loss(output, labels)\n",
        "        # Backward\n",
        "        err.backward()\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        _, train_predict = torch.max(output.data, 1)\n",
        "        train_correct += (train_predict.cpu().numpy() == labels.cpu().numpy()).sum()\n",
        "\n",
        "        # Show training loss at every 200 data points\n",
        "        if (i+1) % 200 == 0:\n",
        "            print( \"[Epoch {} / {}] At step {} of {}, Training loss = {:.4f}\".\n",
        "                format(epoch + 1, epochs, i + 1, len(train_loader), err.item()) )\n",
        "        \n",
        "        # Add to running loss\n",
        "        running_loss[epoch, i] = err.item()\n",
        "\n",
        "    # Accuracy at each epoch: add to tracking\n",
        "    running_train_acc[epoch] = train_correct / len(train_set) * 100\n",
        "\n",
        "    att_model.eval()\n",
        "\n",
        "\n",
        "    for i, (images, labels) in enumerate(tqdm(valid_loader)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = att_model(images).to(device)\n",
        "\n",
        "        _, valid_predict = torch.max(output.data, 1)\n",
        "        valid_correct += (valid_predict.cpu().numpy() == labels.cpu().numpy()).sum()\n",
        "\n",
        "        if epoch == (epochs - 1):\n",
        "            for i in range(len(labels)):\n",
        "                pred_label.append(valid_predict.cpu().numpy()[i])\n",
        "                actual_label.append(labels.cpu().numpy()[i])\n",
        "    \n",
        "    # Accuracy at each epoch: add to tracking, print\n",
        "    running_valid_acc[epoch] = valid_correct / len(valid_set) * 100\n",
        "\n",
        "    print( \"Accuracy of network on test set at epoch {} of {}: {}/{} = {:.2f}%\".\n",
        "          format(epoch + 1, epochs, valid_correct, len(valid_set), (valid_correct / len(valid_set) * 100)) )\n",
        "\n",
        "    if epoch == (epochs - 1):\n",
        "        pd.DataFrame(list(zip(actual_label, pred_label)), columns=['True Label', 'Predicted Label']).to_csv(os.path.join(model_dir, '210525valid_result(att_org_trans).csv'), index=False)\n",
        "\n",
        "    print(\"=================================\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"training time: {:0.1f} sec.\".format(end_time - start_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}